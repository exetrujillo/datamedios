}
)
if (is.null(response)) next
response_data <- httr::content(response, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(response_data$notas)) {
notas <- response_data$notas
# Seleccionar solo las columnas que ya están definidas en all_data
columnas_deseadas <- colnames(all_data)
columnas_existentes <- columnas_deseadas[columnas_deseadas %in% names(notas)]
# Seleccionar y transformar los datos
notas <- notas %>%
dplyr::select(dplyr::all_of(columnas_existentes)) %>%
dplyr::mutate(
dplyr::across(c("year", "month", "day"), as.integer),
dplyr::across(everything(), as.character)
)
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
}
}
# Retornar los datos procesados
return(all_data)
}
extraer_noticias("inteligencia artificial", 100)
extraer_noticias("inteligencia artificial", 100)
data_initial <- dplyr::`%>%`(
httr::content(response_initial, "text", encoding = "UTF-8"),
jsonlite::fromJSON(flatten = TRUE)
)
total_results <- 0
#' Extracción de noticias desde la API de BíoBío.cl
#'
#' Esta función permite realizar una extracción automatizada de noticias desde la API de BíoBío.cl.
#'
#' @param search_query Una frase de búsqueda (obligatoria).
#' @param max_results Número máximo de resultados a extraer (opcional, por defecto todos).
#' @return Un dataframe con las noticias extraídas.
#' @examples
#' noticias <- extraer_noticias("inteligencia artificial", max_results = 100)
#' @export
extraer_noticias <- function(search_query, max_results = NULL) {
# Validamos los parámetros
if (missing(search_query) || !is.character(search_query)) {
stop("Debe proporcionar una frase de búsqueda como texto.")
}
# Inicializamos variables
offset <- 0
total_results <- 0
all_data <- data.frame(
ID = character(),
post_title = character(), #titulo de la nota
post_content = character(), # contenido de la nota
post_excerpt = character(), # resumen breve de la nota
post_URL = character(), # url completa de la nota
post_categories = c(), # categorías asociadas a la nota (lista de objetos con ID, nombre y slug)
post_tags = c(), # etiquetas asociadas a la nota (lista de objetos con ID, nombre y slug)
year = integer(), # año de publicación
month = integer(), # mes de publicación
day = integer(), # día de publicación
post_category_primary.name = character(), #categoría primaria, en biobio son bastante arbitrarias
post_category_secondary.name = character(), #categoría secundaria, en biobio son bastante arbitrarias
post_image.URL = character(), # URL de la imagen destacada.
post_image.alt = character(), # descripción alt de la imagen
post_image.caption = character(), # pie de foto de la imagen.
author.display_name = character(), # nombre del autor de la nota.
resumen_de_ia = character(), # resumen de la nota hecha por ia, si es que aplica
stringsAsFactors = FALSE
)
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode(search_query)),
`Content-Type` = "application/json; charset=UTF-8"
)
# URL inicial
url_initial <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Solicitud inicial para obtener el total de resultados
response_initial <- httr::GET(url_initial, httr::add_headers(.headers = headers))
if (response_initial$status_code == 200) {
data_initial <- httr::content(response_initial, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(data_initial$total)) {
total_results <- as.numeric(data_initial$total)
} else {
stop("No se encontró el parámetro 'total' en la respuesta.")
}
} else {
stop("Error al realizar la solicitud inicial. Código de estado: ", response_initial$status_code)
}
# Limitamos los resultados si max_results está definido
if (!is.null(max_results)) {
total_results <- min(total_results, max_results)
}
# Iteración para obtener todos los datos
while (offset < total_results) {
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
offset <- offset + 20
response <- tryCatch(
{ httr::GET(url, httr::add_headers(.headers = headers)) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
if (is.null(response)) next
response_data <- httr::content(response, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(response_data$notas)) {
notas <- response_data$notas
# Seleccionar solo las columnas que ya están definidas en all_data
columnas_deseadas <- colnames(all_data)
columnas_existentes <- columnas_deseadas[columnas_deseadas %in% names(notas)]
# Seleccionar y transformar los datos
notas <- notas %>%
dplyr::select(dplyr::all_of(columnas_existentes)) %>%
dplyr::mutate(
dplyr::across(c("year", "month", "day"), as.integer),
dplyr::across(everything(), as.character)
)
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
}
}
# Retornar los datos procesados
return(all_data)
}
extraer_noticias("inteligencia artificial", 100)
data_initial <- dplyr::`%>%`(
httr::content(response_initial, "text", encoding = "UTF-8"),
jsonlite::fromJSON(flatten = TRUE)
)
extraer_noticias("inteligencia artificial")
url_ejemplo <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", 20,
"&search=", URLencode("inteligencia artificial"),
"&intervalo=&orden=ultimas"
)
response_ejemplo <- httr::GET(url_initial, httr::add_headers(.headers = headers))
response_ejemplo <- httr::GET(url_ejemplo, httr::add_headers(.headers = headers))
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode("inteligencia_artificial")),
`Content-Type` = "application/json; charset=UTF-8"
)
response_ejemplo <- httr::GET(url_ejemplo, httr::add_headers(.headers = headers))
View(response_ejemplo)
data_initial <- httr::content(response_initial, "text", encoding = "UTF-8") |
jsonlite::fromJSON(flatten = TRUE)
# Solicitud inicial para obtener el total de resultados
response_initial <- httr::GET(url_initial, httr::add_headers(.headers = headers))
# URL inicial
url_initial <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
extraer_noticias("inteligencia artificial")
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
Sys.which("make")
devtools::find_rtools()
library(devtools)
install.packages(devtools)
extraer_noticias("inteligencia artificial")
use_mit_license()
usethis::use_mit_license()
install.packages(usethis)
library("pacman")
library(pacman)
p(usethis)
p_load(usethis)
usethis::use_mit_license()
datamedios::extraer_noticias("inteligencia artificial")
install.packages(c("devtools", "roxygen2", "testthat", "knitr"))
datamedios::extraer_noticias("inteligencia artificial")
use_devtools()
devtools::dev_sitrep()
devtools::load_all()
datamedios::extraer_noticias("inteligencia artificial")
datamedios::extraer_noticias("inteligencia artificial")
install.packages(tidyverse)
install.packages("tidyverse")
datamedios::extraer_noticias("inteligencia artificial")
rlang::last_trace()
rlang::last_trace(drop = FALSE)
datamedios::extraer_noticias("inteligencia artificial")
httr::GET(help())
datamedios::extraer_noticias("inteligencia artificial")
datamedios::extraer_noticias("inteligencia artificial")
data_ejemplo <- datamedios::extraer_noticias("inteligencia artificial")
View(data_ejemplo)
help(datamedios)
help(??datamedios)
man(datamedios)
help(package = "datamedios")
help(package = "datamedios")
noticias <- extraer_noticias("inteligencia artificial", max_results = 100)
View(noticias)
View(data_ejemplo)
help(package = "datamedios")
noticias <- extraer_noticias("arroz", max_results = 100)
View(noticias)
noticias <- extraer_noticias("Siria", max_results = 100)
View(noticias)
noticias$post_content[[1]]
help(package = "datamedios")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
help(package = "datamedios")
help(package = "datamedios")
help(package = "datamedios")
help(package = "datamedios")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-11-20")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-03-12")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-03-12")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-09-12")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2022-11-01", "2023-03-31")
View(noticias)
noticias <- extraer_noticias_fecha("Chatgpt", "2022-11-01", "2023-03-31")
noticias <- extraer_noticias_fecha("Chat gpt", "2022-11-01", "2023-03-31")
noticias <- extraer_noticias_fecha("Chatgpt", "2022-11-01", "2023-03-31")
help(package ="datamedios")
test1 <- init_req_bbcl("inteligencia artificial")
View(test1)
test2 <- init_req_bbcl("inteligencia artificial")
View(test2)
test3 <- datamedios::extraer_noticias_max_res("inteligencia artificial", max_results=400)
test3 <- datamedios::extraer_noticias_max_res("inteligencia artificial", max_results=400)
devtools::install(build_vignettes = FALSE)
.rs.restartR()
library(datamedios)
ls("package:datamedios")
.rs.restartR()
ls("package:datamedios")
ls("package:datamedios")
ls("package:datamedios")
test3 <- datamedios::extraer_noticias_max_res("inteligencia artificial", max_results=400)
test3 <- datamedios::extraer_noticias_max_res("inteligencia artificial", max_results=400)
View(test3)
test4 <- datamedios::extraer_noticias_fecha("inteligencia artificial", "2023-12-12", "2024-03-02")
View(test4)
View(test3)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
limpieza_notas(datos)
sessionInfo()
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
limpieza_notas(datos)
n = c()
class(n)
class("n")
n= c(1, 2, 3, 4)
class(n)
n = ([1:4], pablo, pedro)
n = (1, 2, 3, 4, pablo, pedro)
n = (1, 2, 3, 4, "pablo", "pedro")
n = c(1, 2, 3, 4, "pablo", "pedro")}
n = c(1, 2, 3, 4, "pablo", "pedro")
class(n)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
library(datamedios)
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
limpieza_notas(datos, sinonimos = NULL)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
View(datos)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
View(datos)
limpieza_notas(datos, sinonimos = NULL)
datos_limpios <- limpieza_notas(datos, sinonimos = c())
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
datos_limpios <- limpieza_notas(datos, sinonimos = c())
View(datos_limpios)
View(datos)
View(datos)
datos <- extraer_noticias_fecha("inteligencia artificial", "2016-10-01", "2025-01-01")
View(datos_limpios)
View(datos_limpios)
View(datos)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos_limpios <- limpieza_notas(datos, sinonimos = c())
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos_limpios <- limpieza_notas(datos, sinonimos = c())
datos_limpios <- limpieza_notas(datos, sinonimos = c("AI", "IA"))
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos_limpios <- limpieza_notas(datos, sinonimos = c("AI", "IA"))
stop_words = c("lazy", "8220", "display", "height", "image", "www.biobiochile.cl", "noopener", "png", "320x190", "tambien", "img", "h2", "content", "2024", "09", "label" ,"https" ,"p", "strong", "class", "div", "el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "tu", "siempre", "2", "real", "realidad", "había", "5", "12", "2023", "2021", "muchas", "va", "1", "6", "7", "4", "3", "8", "9", "0")
datamedios::word_cloud(datos, stop_words=stop_words, max_words = 70)
datamedios::grafico_notas_por_mes(datos_limpios, titulo= "saxofon", year_inicio= 2019, month_inicio = 06, year_fin = 2020, month_fin = 01)
datamedios::word_cloud(datos_limpios, stop_words=stop_words, max_words = 70)
datos_limpios <- limpieza_notas(datos, sinonimos = c())
datos_limpios <- limpieza_notas(datos, sinonimos = NULL)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
datos_limpios <- limpieza_notas(datos, sinonimos = c("mangione", "unitedhealthcare"))
View(datos_limpios)
datos <- extraer_noticias_fecha("luigi", "2024-01-01", "2025-01-01")
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-01-01", "2025-01-01")
datos_limpios <- limpieza_notas(datos)
stop_words = c("araucania", "fueron", "hechos", "tambien", "content", "el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "tu", "siempre", "2", "real", "realidad", "había", "5", "12", "2023", "2021", "muchas", "va", "1", "6", "7", "4", "3", "8", "9", "0")
#
resultado <- tabla_frecuencia_palabras(datos, max_words = 5, stop_words = stop_words)
resultado
#
resultado <- tabla_frecuencia_palabras(datos_limpios, max_words = 5, stop_words = stop_words)
resultado
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-01-01", "2025-01-01")
datos_limpios <- limpieza_notas(datos)
stop_words = c("araucania", "fueron", "hechos", "tambien", "content", "el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "tu", "siempre", "2", "real", "realidad", "había", "5", "12", "2023", "2021", "muchas", "va", "1", "6", "7", "4", "3", "8", "9", "0")
#
resultado <- tabla_frecuencia_palabras(datos_limpios, max_words = 5, stop_words = stop_words)
resultado
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-01-01", "2025-01-01")
datos_limpios <- limpieza_notas(datos)
View(datos_limpios)
stop_words = c("araucania", "fueron", "hechos", "tambien", "content", "el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "tu", "siempre", "2", "real", "realidad", "había", "5", "12", "2023", "2021", "muchas", "va", "1", "6", "7", "4", "3", "8", "9", "0")
datamedios::word_cloud(datos_limpios, stop_words=stop_words, max_words = 70)
#
resultado <- tabla_frecuencia_palabras(datos_limpios, max_words = 5, stop_words = stop_words)
resultado
#
resultado <- tabla_frecuencia_palabras(datos_limpios, max_words = 10, stop_words = stop_words)
resultado
#
resultado <- tabla_frecuencia_palabras(datos_limpios, max_words = 20, stop_words = stop_words)
resultado
datamedios::grafico_notas_por_mes(datos, titulo= "Notas por mes luigi mangione", fecha_inicio = "2024-01-01", fecha_fin = "2024-06-30")
datamedios::grafico_notas_por_mes(datos, titulo= "Notas por mes luigi mangione", fecha_inicio = "2024-01-01", fecha_fin = "2024-12-31")
datamedios::grafico_notas_por_mes(datos_limpios, titulo= "Notas por mes luigi mangione", fecha_inicio = "2024-01-01", fecha_fin = "2024-12-31")
datos_parrafos <- datamedios::extracción_parrafos(datos, sinonimos = c("unitedhealthcare"))
View(datos_parrafos)
print(datos_parrafos$parrafos_filtrados[[1]])
print(datos_parrafos$parrafos_filtrados[[2]])
class(datos_parrafos)
class(datos_parrafos$parrafos_filtrados)
class(datos_parrafos$parrafos_filtrados[[2]])
#Cargamos libreria
library(pacman)
p_load(
tidyverse,
httr,
jsonlite,
tidytext,
ggplot2,
ggvenn,
rvest,
stringr,
xml2,
wordcloud2,
arrow,
stopwords,
lubridate,
htmlwidgets
)
rm(list = ls())
# Parámetros básicos
search_query <- "Inteligencia Artificial"  # Palabra clave para obtener los artículos
offset <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results_emol <- 0  # Número total de resultados de la búsqueda
# Definir URL base y parámetros de la consulta
base_url <- "https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes"
size <- 10  # Número de artículos por página
# Inicializar el data frame vacío
all_data_emol <- data.frame(
ID = character(),  # Definimos las columnas vacías
post_content = character(),
post_title = character(),
year = integer(),  # Año como entero
month = integer(),  # Mes como entero
day = integer(),  # Día como entero
seccion = character(),
subSeccion = character(),
stringsAsFactors = FALSE  # Inicializamos el data frame sin factores
)
# Obtener el número total de resultados
url_initial <- paste0(
base_url,
"?q=", URLencode(search_query),
"&size=1&from=0"
)
response_initial <- GET(url_initial)
if (status_code(response_initial) == 200) {
data_initial <- content(response_initial, "text", encoding = "UTF-8") %>%
fromJSON(flatten = TRUE)
if (!is.null(data_initial$hits$total)) {
total_results_emol <- as.numeric(data_initial$hits$total)
message("Número total de resultados: ", total_results_emol)
} else {
stop("No se encontró el parámetro 'total' en la respuesta.")
}
} else {
stop("Error en la solicitud inicial. Código de estado: ", status_code(response_initial))
}
# Iterar por todas las páginas de resultados
while (offset < total_results_emol) {
# Construir la URL para cada iteración
url <- paste0(
base_url,
"?q=", URLencode(search_query),
"&size=", size,
"&from=", offset
)
# Aumentar el offset para la siguiente iteración
offset <- offset + size
# Realizar la solicitud y manejar posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
if (is.null(response)) next
# Procesar el contenido si la respuesta es válida
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Verificar si hay datos en `hits$hits` antes de procesar
if (!is.null(json_data$hits$hits)) {
noticias <- json_data$hits$hits  # Esto es un data.frame, no una lista
# Filtrar los artículos de tipo "noticia"
noticias <- noticias[noticias$`_type` == "noticia", ]
# Extraer datos de manera adecuada desde el data.frame
for (i in 1:nrow(noticias)) {
temp <- noticias[i, ]
# Verificar que temp tenga los datos necesarios antes de procesar
if (!is.null(temp$`_source.id`) && !is.null(temp$`_source.texto`) && !is.null(temp$`_source.titulo`) && !is.null(temp$`_source.fechaPublicacion`)) {
# Extraemos los datos de las columnas adecuadas
all_data_emol <- rbind(all_data_emol, data.frame(
ID = as.character(temp$`_source.id`),
post_content = as.character(temp$`_source.texto`),
post_title = as.character(temp$`_source.titulo`),
year = as.integer(ymd_hms(temp$`_source.fechaPublicacion`) %>% year()),
month = as.integer(ymd_hms(temp$`_source.fechaPublicacion`) %>% month()),
day = as.integer(ymd_hms(temp$`_source.fechaPublicacion`) %>% day()),
seccion = as.character(temp$`_source.seccion`),
subSeccion = as.character(temp$`_source.subSeccion`),
stringsAsFactors = FALSE
))
}
}
}
all_data_emol <- all_data_emol %>%
filter(!is.na(post_content) & post_content != "")
# Mostrar progreso de las iteraciones
# cat(" - Procesados - ", offset, "\n") # Mostrar progreso
}
# Verificar el primer contenido extraído
print(all_data_emol$post_content[160])
# Con expresión regular sacar todo el html resultante dentro del texto
#
total_results_emol <- nrow(all_data_emol)
print(all_data_emol$post_content[10])
print(all_data_emol$post_content[5])
print(all_data_emol$post_content[53])
print(all_data_emol$post_content[59])
print(all_data_emol$post_content[7])
print(all_data_emol$post_content[15])
print(all_data_emol$post_content[130])
View(all_data_emol)
View(data_initial)
View(data_initial[["hits"]][["hits"]])
View(all_data_emol)
View(json_data)
data_initial[["hits"]][["hits"]][[_source.texto]]
data_initial[["hits"]][["hits"]][[_source.texto]][[1]]
data_initial[["hits"]][["hits"]][["_source.texto"]][[1]]
test <- data_initial[["hits"]][["hits"]]
View(test)
View(data_initial)
data_initial[["hits"]][["hits"]][["_source.texto"]]
test_file("tests/testthat/test_limpieza_notas.R")
install.packages("testthat")
test_file("tests/testthat/test_limpieza_notas.R")
testthat::test_file("tests/testthat/test_limpieza_notas.R")
testthat::test_file("datamedios/tests/testthat/test_limpieza_notas.R")
testthat::test_file("GitHub/datamedios/tests/testthat/test_limpieza_notas.R")
dir
testthat::test_file("tests/testthat/test_limpieza_notas.R")
testthat::test_file("testthat/test_limpieza_notas.R")
testthat::test_file("testthat/test_limpieza_notas.R")
testthat::test_file("tests/testthat/test_limpieza_notas.R")
testthat::test_file("tests/testthat/test_limpieza_notas.R")
