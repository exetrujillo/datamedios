hello()
hello <- function() {
print("Hello, world!")
}
hello()
-version
version()
--v
help()
R.Version()
if (!is.null(data$notas)) {
notas <- data$notas
# Seleccionar solo las columnas que ya están definidas en all_data
columnas_deseadas <- colnames(all_data)
columnas_existentes <- columnas_deseadas[columnas_deseadas %in% names(notas)]
# Seleccionar y transformar los datos
notas <- notas %>%
dplyr::select(dplyr::all_of(columnas_existentes)) %>%
dplyr::mutate(
dplyr::across(c("year", "month", "day"), as.integer),
dplyr::across(everything(), as.character)
)
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
}
#' Extracción de noticias desde la API de BíoBío.cl
#'
#' Esta función permite realizar una extracción automatizada de noticias desde la API de BíoBío.cl.
#'
#' @param search_query Una frase de búsqueda (obligatoria).
#' @param max_results Número máximo de resultados a extraer (opcional, por defecto todos).
#' @return Un dataframe con las noticias extraídas.
#' @examples
#' noticias <- extraer_noticias("inteligencia artificial", max_results = 100)
#' @export
extraer_noticias <- function(search_query, max_results = NULL) {
# Validamos los parámetros
if (missing(search_query) || !is.character(search_query)) {
stop("Debe proporcionar una frase de búsqueda como texto.")
}
# Inicializamos variables
offset <- 0
total_results <- 0
all_data <- data.frame(
ID = character(),
post_title = character(), #titulo de la nota
post_content = character(), # contenido de la nota
post_excerpt = character(), # resumen breve de la nota
post_URL = character(), # url completa de la nota
post_categories = c(), # categorías asociadas a la nota (lista de objetos con ID, nombre y slug)
post_tags = c(), # etiquetas asociadas a la nota (lista de objetos con ID, nombre y slug)
year = integer(), # año de publicación
month = integer(), # mes de publicación
day = integer(), # día de publicación
post_category_primary.name = character(), #categoría primaria, en biobio son bastante arbitrarias
post_category_secondary.name = character(), #categoría secundaria, en biobio son bastante arbitrarias
post_image.URL = character(), # URL de la imagen destacada.
post_image.alt = character(), # descripción alt de la imagen
post_image.caption = character(), # pie de foto de la imagen.
author.display_name = character(), # nombre del autor de la nota.
resumen_de_ia = character(), # resumen de la nota hecha por ia, si es que aplica
stringsAsFactors = FALSE
)
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode(search_query)),
`Content-Type` = "application/json; charset=UTF-8"
)
# URL inicial
url_initial <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Solicitud inicial para obtener el total de resultados
response_initial <- httr::GET(url_initial, httr::add_headers(.headers = headers))
if (response_initial$status_code == 200) {
data_initial <- httr::content(response_initial, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(data_initial$total)) {
total_results <- as.numeric(data_initial$total)
} else {
stop("No se encontró el parámetro 'total' en la respuesta.")
}
} else {
stop("Error al realizar la solicitud inicial. Código de estado: ", response_initial$status_code)
}
# Limitamos los resultados si max_results está definido
if (!is.null(max_results)) {
total_results <- min(total_results, max_results)
}
# Iteración para obtener todos los datos
while (offset < total_results) {
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
offset <- offset + 20
response <- tryCatch(
{ httr::GET(url, httr::add_headers(.headers = headers)) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
if (is.null(response)) next
response_data <- httr::content(response, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(response_data$notas)) {
notas <- response_data$notas
# Seleccionar solo las columnas que ya están definidas en all_data
columnas_deseadas <- colnames(all_data)
columnas_existentes <- columnas_deseadas[columnas_deseadas %in% names(notas)]
# Seleccionar y transformar los datos
notas <- notas %>%
dplyr::select(dplyr::all_of(columnas_existentes)) %>%
dplyr::mutate(
dplyr::across(c("year", "month", "day"), as.integer),
dplyr::across(everything(), as.character)
)
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
}
}
# Retornar los datos procesados
return(all_data)
}
extraer_noticias("inteligencia artificial", 100)
extraer_noticias("inteligencia artificial", 100)
data_initial <- dplyr::`%>%`(
httr::content(response_initial, "text", encoding = "UTF-8"),
jsonlite::fromJSON(flatten = TRUE)
)
total_results <- 0
#' Extracción de noticias desde la API de BíoBío.cl
#'
#' Esta función permite realizar una extracción automatizada de noticias desde la API de BíoBío.cl.
#'
#' @param search_query Una frase de búsqueda (obligatoria).
#' @param max_results Número máximo de resultados a extraer (opcional, por defecto todos).
#' @return Un dataframe con las noticias extraídas.
#' @examples
#' noticias <- extraer_noticias("inteligencia artificial", max_results = 100)
#' @export
extraer_noticias <- function(search_query, max_results = NULL) {
# Validamos los parámetros
if (missing(search_query) || !is.character(search_query)) {
stop("Debe proporcionar una frase de búsqueda como texto.")
}
# Inicializamos variables
offset <- 0
total_results <- 0
all_data <- data.frame(
ID = character(),
post_title = character(), #titulo de la nota
post_content = character(), # contenido de la nota
post_excerpt = character(), # resumen breve de la nota
post_URL = character(), # url completa de la nota
post_categories = c(), # categorías asociadas a la nota (lista de objetos con ID, nombre y slug)
post_tags = c(), # etiquetas asociadas a la nota (lista de objetos con ID, nombre y slug)
year = integer(), # año de publicación
month = integer(), # mes de publicación
day = integer(), # día de publicación
post_category_primary.name = character(), #categoría primaria, en biobio son bastante arbitrarias
post_category_secondary.name = character(), #categoría secundaria, en biobio son bastante arbitrarias
post_image.URL = character(), # URL de la imagen destacada.
post_image.alt = character(), # descripción alt de la imagen
post_image.caption = character(), # pie de foto de la imagen.
author.display_name = character(), # nombre del autor de la nota.
resumen_de_ia = character(), # resumen de la nota hecha por ia, si es que aplica
stringsAsFactors = FALSE
)
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode(search_query)),
`Content-Type` = "application/json; charset=UTF-8"
)
# URL inicial
url_initial <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Solicitud inicial para obtener el total de resultados
response_initial <- httr::GET(url_initial, httr::add_headers(.headers = headers))
if (response_initial$status_code == 200) {
data_initial <- httr::content(response_initial, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(data_initial$total)) {
total_results <- as.numeric(data_initial$total)
} else {
stop("No se encontró el parámetro 'total' en la respuesta.")
}
} else {
stop("Error al realizar la solicitud inicial. Código de estado: ", response_initial$status_code)
}
# Limitamos los resultados si max_results está definido
if (!is.null(max_results)) {
total_results <- min(total_results, max_results)
}
# Iteración para obtener todos los datos
while (offset < total_results) {
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
offset <- offset + 20
response <- tryCatch(
{ httr::GET(url, httr::add_headers(.headers = headers)) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
if (is.null(response)) next
response_data <- httr::content(response, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(response_data$notas)) {
notas <- response_data$notas
# Seleccionar solo las columnas que ya están definidas en all_data
columnas_deseadas <- colnames(all_data)
columnas_existentes <- columnas_deseadas[columnas_deseadas %in% names(notas)]
# Seleccionar y transformar los datos
notas <- notas %>%
dplyr::select(dplyr::all_of(columnas_existentes)) %>%
dplyr::mutate(
dplyr::across(c("year", "month", "day"), as.integer),
dplyr::across(everything(), as.character)
)
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
}
}
# Retornar los datos procesados
return(all_data)
}
extraer_noticias("inteligencia artificial", 100)
data_initial <- dplyr::`%>%`(
httr::content(response_initial, "text", encoding = "UTF-8"),
jsonlite::fromJSON(flatten = TRUE)
)
extraer_noticias("inteligencia artificial")
url_ejemplo <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", 20,
"&search=", URLencode("inteligencia artificial"),
"&intervalo=&orden=ultimas"
)
response_ejemplo <- httr::GET(url_initial, httr::add_headers(.headers = headers))
response_ejemplo <- httr::GET(url_ejemplo, httr::add_headers(.headers = headers))
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode("inteligencia_artificial")),
`Content-Type` = "application/json; charset=UTF-8"
)
response_ejemplo <- httr::GET(url_ejemplo, httr::add_headers(.headers = headers))
View(response_ejemplo)
data_initial <- httr::content(response_initial, "text", encoding = "UTF-8") |
jsonlite::fromJSON(flatten = TRUE)
# Solicitud inicial para obtener el total de resultados
response_initial <- httr::GET(url_initial, httr::add_headers(.headers = headers))
# URL inicial
url_initial <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
extraer_noticias("inteligencia artificial")
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
Sys.which("make")
devtools::find_rtools()
library(devtools)
install.packages(devtools)
extraer_noticias("inteligencia artificial")
use_mit_license()
usethis::use_mit_license()
install.packages(usethis)
library("pacman")
library(pacman)
p(usethis)
p_load(usethis)
usethis::use_mit_license()
datamedios::extraer_noticias("inteligencia artificial")
install.packages(c("devtools", "roxygen2", "testthat", "knitr"))
datamedios::extraer_noticias("inteligencia artificial")
use_devtools()
devtools::dev_sitrep()
devtools::load_all()
datamedios::extraer_noticias("inteligencia artificial")
datamedios::extraer_noticias("inteligencia artificial")
install.packages(tidyverse)
install.packages("tidyverse")
datamedios::extraer_noticias("inteligencia artificial")
rlang::last_trace()
rlang::last_trace(drop = FALSE)
datamedios::extraer_noticias("inteligencia artificial")
httr::GET(help())
datamedios::extraer_noticias("inteligencia artificial")
datamedios::extraer_noticias("inteligencia artificial")
data_ejemplo <- datamedios::extraer_noticias("inteligencia artificial")
View(data_ejemplo)
help(datamedios)
help(??datamedios)
man(datamedios)
help(package = "datamedios")
help(package = "datamedios")
noticias <- extraer_noticias("inteligencia artificial", max_results = 100)
View(noticias)
View(data_ejemplo)
help(package = "datamedios")
noticias <- extraer_noticias("arroz", max_results = 100)
View(noticias)
noticias <- extraer_noticias("Siria", max_results = 100)
View(noticias)
noticias$post_content[[1]]
help(package = "datamedios")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
help(package = "datamedios")
help(package = "datamedios")
help(package = "datamedios")
help(package = "datamedios")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-11-20")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-03-12")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-03-12")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-09-12")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2022-11-01", "2023-03-31")
View(noticias)
noticias <- extraer_noticias_fecha("Chatgpt", "2022-11-01", "2023-03-31")
noticias <- extraer_noticias_fecha("Chat gpt", "2022-11-01", "2023-03-31")
noticias <- extraer_noticias_fecha("Chatgpt", "2022-11-01", "2023-03-31")
help(package ="datamedios")
