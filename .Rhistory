hello()
hello <- function() {
print("Hello, world!")
}
hello()
-version
version()
--v
help()
R.Version()
if (!is.null(data$notas)) {
notas <- data$notas
# Seleccionar solo las columnas que ya están definidas en all_data
columnas_deseadas <- colnames(all_data)
columnas_existentes <- columnas_deseadas[columnas_deseadas %in% names(notas)]
# Seleccionar y transformar los datos
notas <- notas %>%
dplyr::select(dplyr::all_of(columnas_existentes)) %>%
dplyr::mutate(
dplyr::across(c("year", "month", "day"), as.integer),
dplyr::across(everything(), as.character)
)
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
}
#' Extracción de noticias desde la API de BíoBío.cl
#'
#' Esta función permite realizar una extracción automatizada de noticias desde la API de BíoBío.cl.
#'
#' @param search_query Una frase de búsqueda (obligatoria).
#' @param max_results Número máximo de resultados a extraer (opcional, por defecto todos).
#' @return Un dataframe con las noticias extraídas.
#' @examples
#' noticias <- extraer_noticias("inteligencia artificial", max_results = 100)
#' @export
extraer_noticias <- function(search_query, max_results = NULL) {
# Validamos los parámetros
if (missing(search_query) || !is.character(search_query)) {
stop("Debe proporcionar una frase de búsqueda como texto.")
}
# Inicializamos variables
offset <- 0
total_results <- 0
all_data <- data.frame(
ID = character(),
post_title = character(), #titulo de la nota
post_content = character(), # contenido de la nota
post_excerpt = character(), # resumen breve de la nota
post_URL = character(), # url completa de la nota
post_categories = c(), # categorías asociadas a la nota (lista de objetos con ID, nombre y slug)
post_tags = c(), # etiquetas asociadas a la nota (lista de objetos con ID, nombre y slug)
year = integer(), # año de publicación
month = integer(), # mes de publicación
day = integer(), # día de publicación
post_category_primary.name = character(), #categoría primaria, en biobio son bastante arbitrarias
post_category_secondary.name = character(), #categoría secundaria, en biobio son bastante arbitrarias
post_image.URL = character(), # URL de la imagen destacada.
post_image.alt = character(), # descripción alt de la imagen
post_image.caption = character(), # pie de foto de la imagen.
author.display_name = character(), # nombre del autor de la nota.
resumen_de_ia = character(), # resumen de la nota hecha por ia, si es que aplica
stringsAsFactors = FALSE
)
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode(search_query)),
`Content-Type` = "application/json; charset=UTF-8"
)
# URL inicial
url_initial <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Solicitud inicial para obtener el total de resultados
response_initial <- httr::GET(url_initial, httr::add_headers(.headers = headers))
if (response_initial$status_code == 200) {
data_initial <- httr::content(response_initial, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(data_initial$total)) {
total_results <- as.numeric(data_initial$total)
} else {
stop("No se encontró el parámetro 'total' en la respuesta.")
}
} else {
stop("Error al realizar la solicitud inicial. Código de estado: ", response_initial$status_code)
}
# Limitamos los resultados si max_results está definido
if (!is.null(max_results)) {
total_results <- min(total_results, max_results)
}
# Iteración para obtener todos los datos
while (offset < total_results) {
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
offset <- offset + 20
response <- tryCatch(
{ httr::GET(url, httr::add_headers(.headers = headers)) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
if (is.null(response)) next
response_data <- httr::content(response, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(response_data$notas)) {
notas <- response_data$notas
# Seleccionar solo las columnas que ya están definidas en all_data
columnas_deseadas <- colnames(all_data)
columnas_existentes <- columnas_deseadas[columnas_deseadas %in% names(notas)]
# Seleccionar y transformar los datos
notas <- notas %>%
dplyr::select(dplyr::all_of(columnas_existentes)) %>%
dplyr::mutate(
dplyr::across(c("year", "month", "day"), as.integer),
dplyr::across(everything(), as.character)
)
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
}
}
# Retornar los datos procesados
return(all_data)
}
extraer_noticias("inteligencia artificial", 100)
extraer_noticias("inteligencia artificial", 100)
data_initial <- dplyr::`%>%`(
httr::content(response_initial, "text", encoding = "UTF-8"),
jsonlite::fromJSON(flatten = TRUE)
)
total_results <- 0
#' Extracción de noticias desde la API de BíoBío.cl
#'
#' Esta función permite realizar una extracción automatizada de noticias desde la API de BíoBío.cl.
#'
#' @param search_query Una frase de búsqueda (obligatoria).
#' @param max_results Número máximo de resultados a extraer (opcional, por defecto todos).
#' @return Un dataframe con las noticias extraídas.
#' @examples
#' noticias <- extraer_noticias("inteligencia artificial", max_results = 100)
#' @export
extraer_noticias <- function(search_query, max_results = NULL) {
# Validamos los parámetros
if (missing(search_query) || !is.character(search_query)) {
stop("Debe proporcionar una frase de búsqueda como texto.")
}
# Inicializamos variables
offset <- 0
total_results <- 0
all_data <- data.frame(
ID = character(),
post_title = character(), #titulo de la nota
post_content = character(), # contenido de la nota
post_excerpt = character(), # resumen breve de la nota
post_URL = character(), # url completa de la nota
post_categories = c(), # categorías asociadas a la nota (lista de objetos con ID, nombre y slug)
post_tags = c(), # etiquetas asociadas a la nota (lista de objetos con ID, nombre y slug)
year = integer(), # año de publicación
month = integer(), # mes de publicación
day = integer(), # día de publicación
post_category_primary.name = character(), #categoría primaria, en biobio son bastante arbitrarias
post_category_secondary.name = character(), #categoría secundaria, en biobio son bastante arbitrarias
post_image.URL = character(), # URL de la imagen destacada.
post_image.alt = character(), # descripción alt de la imagen
post_image.caption = character(), # pie de foto de la imagen.
author.display_name = character(), # nombre del autor de la nota.
resumen_de_ia = character(), # resumen de la nota hecha por ia, si es que aplica
stringsAsFactors = FALSE
)
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode(search_query)),
`Content-Type` = "application/json; charset=UTF-8"
)
# URL inicial
url_initial <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Solicitud inicial para obtener el total de resultados
response_initial <- httr::GET(url_initial, httr::add_headers(.headers = headers))
if (response_initial$status_code == 200) {
data_initial <- httr::content(response_initial, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(data_initial$total)) {
total_results <- as.numeric(data_initial$total)
} else {
stop("No se encontró el parámetro 'total' en la respuesta.")
}
} else {
stop("Error al realizar la solicitud inicial. Código de estado: ", response_initial$status_code)
}
# Limitamos los resultados si max_results está definido
if (!is.null(max_results)) {
total_results <- min(total_results, max_results)
}
# Iteración para obtener todos los datos
while (offset < total_results) {
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
offset <- offset + 20
response <- tryCatch(
{ httr::GET(url, httr::add_headers(.headers = headers)) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
if (is.null(response)) next
response_data <- httr::content(response, "text", encoding = "UTF-8") %>%
jsonlite::fromJSON(flatten = TRUE)
if (!is.null(response_data$notas)) {
notas <- response_data$notas
# Seleccionar solo las columnas que ya están definidas en all_data
columnas_deseadas <- colnames(all_data)
columnas_existentes <- columnas_deseadas[columnas_deseadas %in% names(notas)]
# Seleccionar y transformar los datos
notas <- notas %>%
dplyr::select(dplyr::all_of(columnas_existentes)) %>%
dplyr::mutate(
dplyr::across(c("year", "month", "day"), as.integer),
dplyr::across(everything(), as.character)
)
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
}
}
# Retornar los datos procesados
return(all_data)
}
extraer_noticias("inteligencia artificial", 100)
data_initial <- dplyr::`%>%`(
httr::content(response_initial, "text", encoding = "UTF-8"),
jsonlite::fromJSON(flatten = TRUE)
)
extraer_noticias("inteligencia artificial")
url_ejemplo <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", 20,
"&search=", URLencode("inteligencia artificial"),
"&intervalo=&orden=ultimas"
)
response_ejemplo <- httr::GET(url_initial, httr::add_headers(.headers = headers))
response_ejemplo <- httr::GET(url_ejemplo, httr::add_headers(.headers = headers))
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode("inteligencia_artificial")),
`Content-Type` = "application/json; charset=UTF-8"
)
response_ejemplo <- httr::GET(url_ejemplo, httr::add_headers(.headers = headers))
View(response_ejemplo)
data_initial <- httr::content(response_initial, "text", encoding = "UTF-8") |
jsonlite::fromJSON(flatten = TRUE)
# Solicitud inicial para obtener el total de resultados
response_initial <- httr::GET(url_initial, httr::add_headers(.headers = headers))
# URL inicial
url_initial <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
extraer_noticias("inteligencia artificial")
# Combinar con los datos acumulados
all_data <- dplyr::bind_rows(all_data, notas)
Sys.which("make")
devtools::find_rtools()
library(devtools)
install.packages(devtools)
extraer_noticias("inteligencia artificial")
use_mit_license()
usethis::use_mit_license()
install.packages(usethis)
library("pacman")
library(pacman)
p(usethis)
p_load(usethis)
usethis::use_mit_license()
datamedios::extraer_noticias("inteligencia artificial")
install.packages(c("devtools", "roxygen2", "testthat", "knitr"))
datamedios::extraer_noticias("inteligencia artificial")
use_devtools()
devtools::dev_sitrep()
devtools::load_all()
datamedios::extraer_noticias("inteligencia artificial")
datamedios::extraer_noticias("inteligencia artificial")
install.packages(tidyverse)
install.packages("tidyverse")
datamedios::extraer_noticias("inteligencia artificial")
rlang::last_trace()
rlang::last_trace(drop = FALSE)
datamedios::extraer_noticias("inteligencia artificial")
httr::GET(help())
datamedios::extraer_noticias("inteligencia artificial")
datamedios::extraer_noticias("inteligencia artificial")
data_ejemplo <- datamedios::extraer_noticias("inteligencia artificial")
View(data_ejemplo)
help(datamedios)
help(??datamedios)
man(datamedios)
help(package = "datamedios")
help(package = "datamedios")
noticias <- extraer_noticias("inteligencia artificial", max_results = 100)
View(noticias)
View(data_ejemplo)
help(package = "datamedios")
noticias <- extraer_noticias("arroz", max_results = 100)
View(noticias)
noticias <- extraer_noticias("Siria", max_results = 100)
View(noticias)
noticias$post_content[[1]]
help(package = "datamedios")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
help(package = "datamedios")
help(package = "datamedios")
help(package = "datamedios")
help(package = "datamedios")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-11-20")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2023-12-31")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-03-12")
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-03-12")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2023-01-01", "2024-09-12")
View(noticias)
noticias <- extraer_noticias_fecha("inteligencia artificial", "2022-11-01", "2023-03-31")
View(noticias)
noticias <- extraer_noticias_fecha("Chatgpt", "2022-11-01", "2023-03-31")
noticias <- extraer_noticias_fecha("Chat gpt", "2022-11-01", "2023-03-31")
noticias <- extraer_noticias_fecha("Chatgpt", "2022-11-01", "2023-03-31")
help(package ="datamedios")
test1 <- init_req_bbcl("inteligencia artificial")
View(test1)
test2 <- init_req_bbcl("inteligencia artificial")
View(test2)
test3 <- datamedios::extraer_noticias_max_res("inteligencia artificial", max_results=400)
test3 <- datamedios::extraer_noticias_max_res("inteligencia artificial", max_results=400)
devtools::install(build_vignettes = FALSE)
.rs.restartR()
library(datamedios)
ls("package:datamedios")
.rs.restartR()
ls("package:datamedios")
ls("package:datamedios")
ls("package:datamedios")
test3 <- datamedios::extraer_noticias_max_res("inteligencia artificial", max_results=400)
test3 <- datamedios::extraer_noticias_max_res("inteligencia artificial", max_results=400)
View(test3)
test4 <- datamedios::extraer_noticias_fecha("inteligencia artificial", "2023-12-12", "2024-03-02")
View(test4)
View(test3)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
limpieza_notas(datos)
sessionInfo()
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
limpieza_notas(datos)
n = c()
class(n)
class("n")
n= c(1, 2, 3, 4)
class(n)
n = ([1:4], pablo, pedro)
n = (1, 2, 3, 4, pablo, pedro)
n = (1, 2, 3, 4, "pablo", "pedro")
n = c(1, 2, 3, 4, "pablo", "pedro")}
n = c(1, 2, 3, 4, "pablo", "pedro")
class(n)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
library(datamedios)
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
limpieza_notas(datos, sinonimos = NULL)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
View(datos)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
View(datos)
limpieza_notas(datos, sinonimos = NULL)
datos_limpios <- limpieza_notas(datos, sinonimos = c())
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
datos_limpios <- limpieza_notas(datos, sinonimos = c())
View(datos_limpios)
View(datos)
View(datos)
datos <- extraer_noticias_fecha("inteligencia artificial", "2016-10-01", "2025-01-01")
View(datos_limpios)
View(datos_limpios)
View(datos)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos_limpios <- limpieza_notas(datos, sinonimos = c())
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos_limpios <- limpieza_notas(datos, sinonimos = c())
datos_limpios <- limpieza_notas(datos, sinonimos = c("AI", "IA"))
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos_limpios <- limpieza_notas(datos, sinonimos = c("AI", "IA"))
stop_words = c("lazy", "8220", "display", "height", "image", "www.biobiochile.cl", "noopener", "png", "320x190", "tambien", "img", "h2", "content", "2024", "09", "label" ,"https" ,"p", "strong", "class", "div", "el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "tu", "siempre", "2", "real", "realidad", "había", "5", "12", "2023", "2021", "muchas", "va", "1", "6", "7", "4", "3", "8", "9", "0")
datamedios::word_cloud(datos, stop_words=stop_words, max_words = 70)
datamedios::grafico_notas_por_mes(datos_limpios, titulo= "saxofon", year_inicio= 2019, month_inicio = 06, year_fin = 2020, month_fin = 01)
datamedios::word_cloud(datos_limpios, stop_words=stop_words, max_words = 70)
datos_limpios <- limpieza_notas(datos, sinonimos = c())
datos_limpios <- limpieza_notas(datos, sinonimos = NULL)
devtools::load_all("C:/Users/ismae/Documents/GitHub/datamedios")
datos <- extraer_noticias_fecha("luigi", "2024-10-01", "2025-01-01")
datos_limpios <- limpieza_notas(datos, sinonimos = c("mangione", "unitedhealthcare"))
View(datos_limpios)
